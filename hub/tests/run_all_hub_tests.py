#!/usr/bin/env python3
# run_all_hub_tests.py - Hubæ¨¡å—å…¨é¢å¼‚æ­¥æµ‹è¯•è¿è¡Œå™¨
# æµ‹è¯•ç›®æ ‡ï¼šç»Ÿä¸€è¿è¡Œæ‰€æœ‰hubå¼‚æ­¥æµ‹è¯•è„šæœ¬ï¼Œç”Ÿæˆæ±‡æ€»æŠ¥å‘Š

import sys
import os
import json
import time
import asyncio
import subprocess
from typing import Dict, Any, List

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°Pythonè·¯å¾„ä¸­ï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥é¡¹ç›®æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))

async def run_single_test_async(test_script_path: str) -> Dict[str, Any]:
    """
    å¼‚æ­¥è¿è¡Œå•ä¸ªæµ‹è¯•è„šæœ¬å¹¶æ”¶é›†ç»“æœ
    ç›®çš„ï¼šæ‰§è¡ŒæŒ‡å®šçš„å¼‚æ­¥æµ‹è¯•è„šæœ¬å¹¶è§£æå…¶è¾“å‡ºç»“æœ

    å‚æ•°:
        test_script_path: æµ‹è¯•è„šæœ¬çš„æ–‡ä»¶è·¯å¾„

    è¿”å›:
        åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
    """
    test_name = os.path.basename(test_script_path).replace(".py", "")

    print(f"\n{'='*60}")
    print(f"å¼‚æ­¥è¿è¡Œæµ‹è¯•: {test_name}")
    print(f"{'='*60}")

    start_time = time.time()

    try:
        # ä½¿ç”¨asyncio.subprocesså¼‚æ­¥æ‰§è¡Œæµ‹è¯•è„šæœ¬
        process = await asyncio.create_subprocess_exec(
            sys.executable, test_script_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=os.path.dirname(os.path.dirname(__file__))  # åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
        )

        try:
            stdout, stderr = await asyncio.wait_for(
                process.communicate(),
                timeout=300.0  # 5åˆ†é’Ÿè¶…æ—¶
            )
            execution_time = time.time() - start_time

            # è§£ææµ‹è¯•ç»“æœ
            stdout_lines = stdout.decode().split('\n') if stdout else []
            stderr_lines = stderr.decode().split('\n') if stderr else []

            # æŸ¥æ‰¾æœ€ç»ˆç»“æœ
            final_result = "UNKNOWN"
            for line in reversed(stdout_lines):
                if "FINAL RESULT:" in line:
                    final_result = line.split("FINAL RESULT:")[-1].strip()
                    break

            # ç»Ÿè®¡å¼‚å¸¸æ•°é‡
            exception_count = sum(1 for line in stdout_lines if "EXCEPTION:" in line)

            # ç»Ÿè®¡æµ‹è¯•é˜¶æ®µ
            phase_count = sum(1 for line in stdout_lines if "Phase" in line and "===" in line)

            return {
                "test_name": test_name,
                "success": final_result == "TEST COMPLETED" and process.returncode == 0,
                "final_result": final_result,
                "return_code": process.returncode,
                "execution_time": round(execution_time, 2),
                "exception_count": exception_count,
                "phase_count": phase_count,
                "stdout_lines": len(stdout_lines),
                "stderr_lines": len(stderr_lines),
                "has_output": len(stdout_lines) > 0,
                "stdout_sample": stdout_lines[:3] if stdout_lines else [],
                "stderr_sample": stderr_lines[:3] if stderr_lines else [],
                "execution_mode": "async"
            }

        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            process.kill()
            return {
                "test_name": test_name,
                "success": False,
                "final_result": "TIMEOUT",
                "return_code": -1,
                "execution_time": round(execution_time, 2),
                "exception_count": 0,
                "phase_count": 0,
                "error": "Async test execution timeout (5 minutes)",
                "execution_mode": "async"
            }

    except Exception as e:
        execution_time = time.time() - start_time
        return {
            "test_name": test_name,
            "success": False,
            "final_result": "ERROR",
            "return_code": -1,
            "execution_time": round(execution_time, 2),
            "exception_count": 0,
            "phase_count": 0,
            "error": str(e)
        }

def generate_test_report(test_results: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    ç”Ÿæˆæµ‹è¯•æ±‡æ€»æŠ¥å‘Š
    ç›®çš„ï¼šåŸºäºæ‰€æœ‰æµ‹è¯•ç»“æœç”Ÿæˆè¯¦ç»†çš„æ±‡æ€»åˆ†æ
    
    å‚æ•°:
        test_results: æ‰€æœ‰æµ‹è¯•çš„ç»“æœåˆ—è¡¨
    
    è¿”å›:
        æ±‡æ€»æŠ¥å‘Šå­—å…¸
    """
    total_tests = len(test_results)
    successful_tests = sum(1 for result in test_results if result.get("success", False))
    failed_tests = total_tests - successful_tests
    
    total_execution_time = sum(result.get("execution_time", 0) for result in test_results)
    total_exceptions = sum(result.get("exception_count", 0) for result in test_results)
    total_phases = sum(result.get("phase_count", 0) for result in test_results)
    
    # åˆ†ç±»æµ‹è¯•ç»“æœ
    passed_tests = [r for r in test_results if r.get("success", False)]
    failed_test_details = [r for r in test_results if not r.get("success", False)]
    
    # æ€§èƒ½åˆ†æ
    avg_execution_time = total_execution_time / total_tests if total_tests > 0 else 0
    fastest_test = min(test_results, key=lambda x: x.get("execution_time", float('inf')))
    slowest_test = max(test_results, key=lambda x: x.get("execution_time", 0))
    
    return {
        "test_summary": {
            "total_tests": total_tests,
            "successful_tests": successful_tests,
            "failed_tests": failed_tests,
            "success_rate": round(successful_tests / total_tests * 100, 2) if total_tests > 0 else 0
        },
        "execution_summary": {
            "total_execution_time": round(total_execution_time, 2),
            "average_execution_time": round(avg_execution_time, 2),
            "fastest_test": {
                "name": fastest_test.get("test_name", "unknown"),
                "time": fastest_test.get("execution_time", 0)
            },
            "slowest_test": {
                "name": slowest_test.get("test_name", "unknown"), 
                "time": slowest_test.get("execution_time", 0)
            }
        },
        "quality_metrics": {
            "total_exceptions_handled": total_exceptions,
            "total_test_phases": total_phases,
            "average_exceptions_per_test": round(total_exceptions / total_tests, 1) if total_tests > 0 else 0,
            "average_phases_per_test": round(total_phases / total_tests, 1) if total_tests > 0 else 0
        },
        "passed_tests": [
            {
                "name": test["test_name"],
                "time": test["execution_time"],
                "phases": test["phase_count"]
            }
            for test in passed_tests
        ],
        "failed_tests": [
            {
                "name": test["test_name"],
                "final_result": test.get("final_result", "UNKNOWN"),
                "error": test.get("error", "No specific error"),
                "execution_time": test.get("execution_time", 0)
            }
            for test in failed_test_details
        ],
        "recommendations": generate_recommendations(test_results)
    }

def generate_recommendations(test_results: List[Dict[str, Any]]) -> List[str]:
    """
    ç”Ÿæˆæµ‹è¯•ç»“æœå»ºè®®
    ç›®çš„ï¼šåŸºäºæµ‹è¯•ç»“æœç”Ÿæˆæ”¹è¿›å»ºè®®
    
    å‚æ•°:
        test_results: æ‰€æœ‰æµ‹è¯•ç»“æœåˆ—è¡¨
    
    è¿”å›:
        å»ºè®®åˆ—è¡¨
    """
    recommendations = []
    
    total_tests = len(test_results)
    successful_tests = sum(1 for r in test_results if r.get("success", False))
    success_rate = successful_tests / total_tests if total_tests > 0 else 0
    
    # æˆåŠŸç‡å»ºè®®
    if success_rate == 1.0:
        recommendations.append("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Hubæ¨¡å—æ¶æ„å‡çº§æˆåŠŸï¼Œç³»ç»Ÿç¨³å®šæ€§ä¼˜ç§€")
    elif success_rate >= 0.8:
        recommendations.append("âœ… å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œç³»ç»ŸåŸºæœ¬ç¨³å®šï¼Œéœ€è¦ä¿®å¤å°‘é‡é—®é¢˜")
    elif success_rate >= 0.6:
        recommendations.append("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦é‡ç‚¹å…³æ³¨å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
    else:
        recommendations.append("ğŸš¨ å¤šä¸ªæµ‹è¯•å¤±è´¥ï¼Œå»ºè®®å…¨é¢æ£€æŸ¥Hubæ¨¡å—å®ç°")
    
    # æ€§èƒ½å»ºè®®
    avg_time = sum(r.get("execution_time", 0) for r in test_results) / total_tests
    if avg_time > 30:
        recommendations.append("â±ï¸ æµ‹è¯•æ‰§è¡Œæ—¶é—´è¾ƒé•¿ï¼Œè€ƒè™‘ä¼˜åŒ–æ€§èƒ½æˆ–å¹¶è¡Œæ‰§è¡Œ")
    elif avg_time < 5:
        recommendations.append("âš¡ æµ‹è¯•æ‰§è¡Œé€Ÿåº¦ä¼˜ç§€ï¼Œç³»ç»Ÿå“åº”å¿«é€Ÿ")
    
    # å¼‚å¸¸å¤„ç†å»ºè®®
    total_exceptions = sum(r.get("exception_count", 0) for r in test_results)
    if total_exceptions == 0:
        recommendations.append("ğŸ›¡ï¸ å¼‚å¸¸å¤„ç†å®Œå–„ï¼Œç³»ç»Ÿé²æ£’æ€§è‰¯å¥½")
    elif total_exceptions > 20:
        recommendations.append("ğŸ”§ æ£€æµ‹åˆ°å¤§é‡å¼‚å¸¸ï¼Œå»ºè®®ä¼˜åŒ–å¼‚å¸¸å¤„ç†æœºåˆ¶")
    
    # å¤±è´¥æµ‹è¯•å…·ä½“å»ºè®®
    failed_tests = [r for r in test_results if not r.get("success", False)]
    if failed_tests:
        recommendations.append(f"ğŸ” é‡ç‚¹å…³æ³¨å¤±è´¥çš„æµ‹è¯•ï¼š{', '.join(t['test_name'] for t in failed_tests)}")
    
    return recommendations

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œæ‰€æœ‰Hubæµ‹è¯•å¹¶ç”ŸæˆæŠ¥å‘Š
    """
    print("ğŸš€ å¼€å§‹æ‰§è¡ŒHubæ¨¡å—å…¨é¢æµ‹è¯•å¥—ä»¶")
    print(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å®šä¹‰æ‰€æœ‰æµ‹è¯•è„šæœ¬
    test_scripts = [
        "hub/test_hub_module_registration.py",
        "hub/test_hub_intent_routing.py", 
        "hub/test_hub_flow_integrity.py",
        "hub/test_hub_exception_handling.py",
        "hub/test_hub_system_integration.py"
    ]
    
    # éªŒè¯æµ‹è¯•è„šæœ¬æ˜¯å¦å­˜åœ¨
    existing_scripts = []
    for script in test_scripts:
        if os.path.exists(script):
            existing_scripts.append(script)
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šæµ‹è¯•è„šæœ¬ä¸å­˜åœ¨ - {script}")
    
    if not existing_scripts:
        print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•æµ‹è¯•è„šæœ¬")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(existing_scripts)} ä¸ªæµ‹è¯•è„šæœ¬")
    print("REQUEST DATA:")
    print(json.dumps({
        "action": "run_all_hub_tests",
        "total_scripts": len(existing_scripts),
        "test_scripts": [os.path.basename(script) for script in existing_scripts]
    }, indent=2))
    
    # æ‰§è¡Œæ‰€æœ‰æµ‹è¯•
    all_test_results = []
    overall_start_time = time.time()
    
    for script_path in existing_scripts:
        # ä½¿ç”¨å¼‚æ­¥æµ‹è¯•è¿è¡Œå™¨æ‰§è¡Œå•ä¸ªæµ‹è¯•è„šæœ¬
        test_result = asyncio.run(run_single_test_async(script_path))
        all_test_results.append(test_result)

        # æ˜¾ç¤ºå•ä¸ªæµ‹è¯•çš„ç®€è¦ç»“æœ
        status_icon = "âœ…" if test_result["success"] else "âŒ"
        print(f"{status_icon} {test_result['test_name']}: {test_result['final_result']} ({test_result['execution_time']}s)")
    
    overall_execution_time = time.time() - overall_start_time
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    # generate_test_report é€šè¿‡è°ƒç”¨ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š
    summary_report = generate_test_report(all_test_results)
    
    # è¾“å‡ºæ±‡æ€»æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ğŸ† Hubæ¨¡å—æµ‹è¯•æ±‡æ€»æŠ¥å‘Š")
    print(f"{'='*80}")
    
    print("RESPONSE DATA:")
    print(json.dumps({
        "overall_execution_time": round(overall_execution_time, 2),
        "test_environment": {
            "python_version": sys.version.split()[0],
            "platform": sys.platform,
            "working_directory": os.getcwd()
        },
        "summary_report": summary_report
    }, indent=2))
    
    # æ˜¾ç¤ºå»ºè®®
    print(f"\nğŸ“‹ æµ‹è¯•å»ºè®®:")
    for i, recommendation in enumerate(summary_report["recommendations"], 1):
        print(f"{i}. {recommendation}")
    
    # ç¡®å®šæœ€ç»ˆç»“æœ
    success_rate = summary_report["test_summary"]["success_rate"]
    if success_rate == 100:
        print("\nğŸ‰ FINAL RESULT: ALL TESTS PASSED")
        sys.exit(0)
    elif success_rate >= 80:
        print(f"\nâš ï¸ FINAL RESULT: MOSTLY PASSED ({success_rate}% success rate)")
        sys.exit(0)
    else:
        print(f"\nâŒ FINAL RESULT: TESTS FAILED ({success_rate}% success rate)")
        sys.exit(1)

if __name__ == "__main__":
    main()
